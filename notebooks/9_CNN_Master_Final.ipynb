{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10_CNN_Master_Final.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('825': venv)","metadata":{"interpreter":{"hash":"59650d09dba9c4f7c5c1641a7604d868492dda203335bf6b3bcf6f068be5d520"}}},"language_info":{"name":"python","version":"3.8.8-final"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"et2PV0nl1_Vs","executionInfo":{"status":"ok","timestamp":1617751270924,"user_tz":240,"elapsed":3802,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","import torch\n","\n","# torch.cuda.empty_cache()"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zz-pw0mQ7rrM","executionInfo":{"status":"ok","timestamp":1617751271781,"user_tz":240,"elapsed":250,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["class Identity(torch.nn.Module):\n","    def __init__(self):\n","        super(Identity, self).__init__()\n","\n","    def forward(self, x):\n","        return x"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKncqEopbr1y","executionInfo":{"status":"ok","timestamp":1617751293220,"user_tz":240,"elapsed":263,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["# Setting this to True uses a CNN archetecture with 2 fully connected layers.\n","# If False there are 300 features (len(filter_sizes) * n_filters)\n","DUAL_FC_LAYERS = True\n","# The features are extracted from the output of the first layer (150 features per model)\n","NUM_FEATURES = 150"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"EYIA9hGCs1Ds","executionInfo":{"status":"ok","timestamp":1617751296973,"user_tz":240,"elapsed":270,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","class CNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes,\n","                 output_dim, dropout, pad_idx):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","        self.convs = nn.ModuleList([\n","                                    nn.Conv2d(in_channels=1,\n","                                              out_channels=n_filters, \n","                                              kernel_size=(fs, embedding_dim))\n","                                    for fs in filter_sizes\n","                                    ])\n","        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","        # self.fc = Identity()\n","        # self.dropout = Identity()\n","\n","    def forward(self, text):\n","        # text = [batch size, sent len]\n","        embedded = self.embedding(text)\n","\n","        # embedded = [batch size, sent len, emb dim]\n","        embedded = embedded.unsqueeze(1)\n","\n","        # embedded = [batch size, 1, sent len, emb dim]\n","        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n","\n","        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n","        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n","\n","        cat = self.dropout(torch.cat(pooled, dim=1))\n","\n","        return self.fc(cat)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1axthWCp-dw","executionInfo":{"status":"ok","timestamp":1617754078794,"user_tz":240,"elapsed":266,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["class ECE(nn.Module):\n","\n","    def __init__(self, n_bins=15):\n","        \"\"\"\n","        n_bins (int): number of confidence interval bins\n","        \"\"\"\n","        super(ECE, self).__init__()\n","        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n","        self.bin_lowers = bin_boundaries[:-1]\n","        self.bin_uppers = bin_boundaries[1:]\n","\n","    def forward(self, logits, labels):\n","        rounded_preds = torch.round(torch.sigmoid(logits))\n","        softmaxes = F.softmax(logits, dim=1)\n","        confidences, predictions = torch.max(softmaxes, dim=1)\n","        accuracies = predictions.eq(labels)\n","        ece = torch.zeros(1, device=logits.device)\n","        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n","            # Calculated |confidence - accuracy| in each bin\n","            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n","            prop_in_bin = in_bin.float().mean()\n","            if prop_in_bin.item() > 0:\n","                accuracy_in_bin = accuracies[in_bin].float().mean()\n","                avg_confidence_in_bin = confidences[in_bin].mean()\n","                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n","        return ece"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"ajaaX8Rfaiwi","executionInfo":{"status":"ok","timestamp":1617751311056,"user_tz":240,"elapsed":253,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","class CNN_2FC(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes,\n","                 output_dim, dropout, pad_idx):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","        self.convs = nn.ModuleList([\n","                                    nn.Conv2d(in_channels=1,\n","                                              out_channels=n_filters, \n","                                              kernel_size=(fs, embedding_dim))\n","                                    for fs in filter_sizes\n","                                    ])\n","        self.fc1 = nn.Linear(len(filter_sizes) * n_filters, NUM_FEATURES)\n","        self.fc = nn.Linear(NUM_FEATURES, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","        # self.fc = Identity()\n","        # self.dropout = Identity()\n","\n","    def forward(self, text):\n","        # text = [batch size, sent len]\n","        embedded = self.embedding(text)\n","\n","        # embedded = [batch size, sent len, emb dim]\n","        embedded = embedded.unsqueeze(1)\n","\n","        # embedded = [batch size, 1, sent len, emb dim]\n","        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n","\n","        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n","        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n","\n","        cat = self.dropout(torch.cat(pooled, dim=1))\n","\n","        return self.fc(self.fc1(cat))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5bgxBoWOITL","executionInfo":{"status":"ok","timestamp":1617751313409,"user_tz":240,"elapsed":287,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["class SUPER_CNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes,\n","                 output_dim, dropout, pad_idx, use_base=False, num_features=300):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","        self.convs = nn.ModuleList([\n","                                    nn.Conv2d(in_channels=1,\n","                                              out_channels=n_filters, \n","                                              kernel_size=(fs, embedding_dim))\n","                                    for fs in filter_sizes\n","                                    ])\n","\n","        # self.sentiment_model = CNN(vocab_size, embedding_dim, n_filters, filter_sizes,\n","        #          output_dim, dropout, pad_idx)\n","        # self.sentiment_model.load_state_dict(torch.load('CNN-sentiment.pt'))\n","        self.use_base = use_base\n","        \n","        if self.use_base:\n","            self.sarcasm_base_model = torch.load('CNN-sarcasm-base.pt')\n","            for param in self.sarcasm_base_model.parameters():\n","                param.requires_grad = False\n","            self.sarcasm_base_model.embedding = Identity()\n","            self.sarcasm_base_model.fc = Identity()\n","            self.sarcasm_base_model.dropout = Identity()\n","        \n","\n","        self.sentiment_model = torch.load('CNN-sentiment.pt', map_location=torch.device('cpu'))\n","        for param in self.sentiment_model.parameters():\n","          param.requires_grad = False\n","        self.sentiment_model.embedding = Identity()\n","        self.sentiment_model.fc = Identity()\n","        self.sentiment_model.dropout = Identity()\n","        \n","        # self.emotion_model = CNN(vocab_size, embedding_dim, n_filters, filter_sizes,\n","        #          5, dropout, pad_idx)\n","        # self.emotion_model.load_state_dict(torch.load('CNN-emotion.pt'))\n","        self.emotion_model = torch.load('CNN-emotion.pt', map_location=torch.device('cpu'))\n","        for param in self.emotion_model.parameters():\n","          param.requires_grad = False\n","        self.emotion_model.embedding = Identity()\n","        self.emotion_model.fc = Identity()\n","        self.emotion_model.dropout = Identity()\n","\n","        # self.formality_model = CNN(vocab_size, embedding_dim, n_filters, filter_sizes,\n","        #          output_dim, dropout, pad_idx)\n","        # self.formality_model.load_state_dict(torch.load('CNN-formality-binary.pt'))\n","        self.formality_model = torch.load('CNN-formality-binary.pt', map_location=torch.device('cpu'))\n","        for param in self.formality_model.parameters():\n","          param.requires_grad = False\n","        self.formality_model.embedding = Identity()\n","        self.formality_model.fc = Identity()\n","        self.formality_model.dropout = Identity()\n","\n","        # self.informativeness_model = CNN(vocab_size, embedding_dim, n_filters, filter_sizes,\n","        #          output_dim, dropout, pad_idx)\n","        # self.informativeness_model.load_state_dict(torch.load('CNN-informativeness-binary.pt'))\n","        self.informativeness_model = torch.load('CNN-informativeness-binary.pt', map_location=torch.device('cpu'))\n","        for param in self.informativeness_model.parameters():\n","          param.requires_grad = False\n","        self.informativeness_model.embedding = Identity()\n","        self.informativeness_model.fc = Identity()\n","        self.informativeness_model.dropout = Identity()\n","        \n","        if DUAL_FC_LAYERS:\n","            self.fc1 = nn.Linear(len(filter_sizes)*n_filters, NUM_FEATURES)\n","            self.fc = nn.Linear(5 * NUM_FEATURES, output_dim)\n","        else:\n","            self.fc = nn.Linear(5 * len(filter_sizes)*n_filters, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, text):\n","        # text = [batch size, sent len]\n","        embedded = self.embedding(text)\n","\n","        # embedded = [batch size, sent len, emb dim]\n","        # embedded = embedded.unsqueeze(1)\n","\n","        # embedded = [batch size, 1, sent len, emb dim]\n","        \n","        # print(\"TESTING\")\n","        o1 = self.sentiment_model(embedded)\n","        o2 = self.emotion_model(embedded)\n","        o3 = self.formality_model(embedded)\n","        o4 = self.informativeness_model(embedded)\n","        if self.use_base:\n","            o5 = self.sarcasm_base_model(embedded)\n","            cat = self.dropout(torch.cat((o1, o2, o3, o4, o5), dim=1))\n","        else:\n","            conved = [F.relu(conv(embedded.unsqueeze(1))).squeeze(3) for conv in self.convs]\n","            # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n","            pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n","            pooled = torch.cat(pooled,dim=1)\n","            if DUAL_FC_LAYERS:\n","                pooled = self.fc1(pooled)\n","            cat = self.dropout(torch.cat((pooled, o1, o2, o3, o4), dim=1))\n","        # print(\"TESTING 2\")\n","        # print(pooled.type)\n","        # print(o1.type)\n","        # print(o2.type)\n","        \n","        # print(cat.shape)\n","\n","        return self.fc(cat)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MR2ivlR0NPj4"},"source":["## Sarcasm Model Definition"]},{"cell_type":"code","metadata":{"id":"EbrlT6PwqutK","executionInfo":{"status":"ok","timestamp":1617756859981,"user_tz":240,"elapsed":949,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["from torchtext.legacy import data\n","from torchtext.legacy import datasets\n","import torch.optim as optim\n","import random\n","import numpy as np\n","import time\n","from sklearn.metrics import f1_score\n","SEED = 1234\n","\n","\n","class ModelTrainer():\n","\n","    def __init__(self, model_name, model_type=None, text_field_name=None,\n","                 label_field_name=None, IMDB=False, train_file_name=None,\n","                 test_file_name=None, embedding_dim=100, criterion_str=\"CE\",\n","                 max_vocab_size=25000, num_filters=100, filter_sizes=[3, 4, 5],\n","                 dropout=0.5, output_dim=1, batch_size=64, dtype=torch.long, use_base=False, num_features=300):\n","        self.model_name = model_name\n","        self.model_type = model_type\n","        self.text_field_name = text_field_name\n","        self.label_field_name = label_field_name\n","        self.IMDB = IMDB\n","        self.train_file_name = train_file_name\n","        self.test_file_name = test_file_name\n","        self.criterion_str = criterion_str\n","        self.embedding_dim = embedding_dim\n","        self.max_vocab_size = max_vocab_size\n","        self.num_filters = num_filters\n","        self.filter_sizes = filter_sizes\n","        self.dropout = dropout\n","        self.batch_size = batch_size\n","        self.dtype = dtype\n","        self.use_base = use_base\n","\n","    def load_dataset(self):\n","        \"\"\" Loads the dataset using torchtext\n","        \"\"\"\n","        r_seed = random.seed(SEED)\n","        np.random.seed(SEED)\n","        torch.manual_seed(SEED)\n","        torch.backends.cudnn.deterministic = True\n","\n","        self.TEXT = data.Field(\n","            tokenize='spacy',\n","            tokenizer_language='en_core_web_sm',\n","            batch_first=True)\n","        self.LABEL = data.LabelField(dtype=self.dtype) # dtype=torch.float)\n","        # LOAD DATASET\n","        if self.IMDB:\n","            self.dataset, test_data = datasets.IMDB.splits(self.TEXT, self.LABEL)\n","            train_data, valid_data = self.dataset.split(random_state=r_seed)\n","        else:\n","            if self.text_field_name == \"Sentence\":\n","                # Formality and Informativeness don't work with dict fields\n","                # tuples representative of tabular format\n","                fields = [\n","                          (self.label_field_name, self.LABEL),\n","                          (self.text_field_name, self.TEXT)\n","                ]\n","                skip_header = True\n","            else:\n","                fields = {\n","                    self.label_field_name: (self.label_field_name, self.LABEL),\n","                    self.text_field_name: (self.text_field_name, self.TEXT)\n","                }\n","                skip_header = False\n","            format = self.train_file_name.split('.')[-1]\n","            if self.test_file_name:\n","                \n","                self.dataset, test_data = data.TabularDataset.splits(\n","                    path='/content',\n","                    train=self.train_file_name,\n","                    test=self.test_file_name,\n","                    format=format,\n","                    fields=fields,\n","                    skip_header=skip_header\n","                )\n","                train_data, valid_data = self.dataset.split(random_state=r_seed)\n","            else:\n","                self.dataset = data.TabularDataset.splits(\n","                    path='/content',\n","                    train=self.train_file_name,\n","                    format=format,\n","                    fields=fields,\n","                    skip_header=skip_header\n","                )[0]\n","                train_data, valid_data, test_data = self.dataset.split(\n","                    split_ratio=[0.4, 0.3, 0.3], random_state=r_seed)\n","\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        if torch.cuda.is_available():\n","          print(torch.cuda.get_device_name(0))\n","\n","        self.train_iter, self.val_iter, self.test_iter = data.BucketIterator.splits(\n","            (train_data, valid_data, test_data),\n","            batch_size=self.batch_size,\n","            device=self.device,\n","            sort=False\n","            )\n","\n","    def build_vocab(self):\n","        \"\"\" Builds the vocabulary of the dataset using torchtext\n","        \"\"\"\n","        self.TEXT.build_vocab(\n","            self.dataset,\n","            max_size=self.max_vocab_size,\n","            vectors=\"glove.6B.100d\",\n","            unk_init=torch.Tensor.normal_\n","        )\n","        self.LABEL.build_vocab(self.dataset)\n","\n","    def init_model(self):\n","        \"\"\" Create CNN model with the supplied/derived parameters.\n","            Loads \"glove.6B.100d\" weights into the model's embedding layer\n","        \"\"\"\n","        # Get the size of the vocabulary\n","        vocab_size = len(self.TEXT.vocab)\n","        if self.criterion_str == \"CE\":\n","          output_dim = len(self.LABEL.vocab)\n","        else:\n","          output_dim = 1\n","        pad_idx = self.TEXT.vocab.stoi[self.TEXT.pad_token]\n","        # Initialize model\n","        if self.model_type == 'deep':\n","          self.model = VDCNN(output_dim, vocab_size, pad_idx,\n","                             self.embedding_dim, shortcut=True)\n","        elif self.model_type == 'master':\n","          self.model = SUPER_CNN(vocab_size, self.embedding_dim, \n","                                 self.num_filters, self.filter_sizes, \n","                                 output_dim, self.dropout, pad_idx, use_base=self.use_base)\n","        else:\n","          if DUAL_FC_LAYERS:\n","            self.model = CNN_2FC(vocab_size, self.embedding_dim, self.num_filters,\n","                                 self.filter_sizes, output_dim, self.dropout,\n","                                 pad_idx)\n","          else:\n","            self.model = CNN(vocab_size, self.embedding_dim, self.num_filters,\n","                             self.filter_sizes, output_dim, self.dropout,\n","                             pad_idx)\n","        # Get pretrained weights from vocab\n","        pretrained_embeddings = self.TEXT.vocab.vectors\n","        self.model.embedding.weight.data.copy_(pretrained_embeddings)\n","\n","        # zero initial weights of <unk> and <pad>\n","        unk_index = self.TEXT.vocab.stoi[self.TEXT.unk_token]\n","\n","        self.model.embedding.weight.data[unk_index] = torch.zeros(self.embedding_dim)\n","        self.model.embedding.weight.data[pad_idx] = torch.zeros(self.embedding_dim)\n","\n","    def init_optimizer(self):\n","        \"\"\" Initializes the optimizor (Adam) and the criterion (BCEWithLogitsLoss)\n","        \"\"\"\n","        self.optimizer = optim.Adam(self.model.parameters())\n","        self.model = self.model.to(self.device)\n","        if self.criterion_str == \"CE\":\n","            self.criterion = nn.CrossEntropyLoss()\n","        elif self.criterion_str == \"BCE\":\n","            self.criterion = nn.BCEWithLogitsLoss()\n","        self.criterion = self.criterion.to(self.device)\n","\n","    def train_model(self, num_epochs):\n","        \"\"\" Trains and validates the model (and prints the results) for the\n","            given number of epochs. Saves the model from the epoch which\n","            yeilded the lowest validation loss.\n","\n","        Args:\n","            num_epochs (int): number of epochs to train the model\n","        \"\"\"\n","        print(self.model_name)\n","        best_valid_loss = float('inf')\n","\n","        for epoch in range(num_epochs):\n","            start_time = time.time()\n","\n","            train_loss, train_acc = self.train_epoch()\n","            valid_loss, valid_acc, _, _ = self.evaluate_epoch(self.val_iter)\n","\n","            end_time = time.time()\n","\n","            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","            if valid_loss < best_valid_loss:\n","                best_valid_loss = valid_loss\n","                # torch.save(self.model.state_dict(), f'{self.model_name}.pt')\n","                torch.save(self.model, f'{self.model_name}.pt')\n","\n","            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","    def test_model(self):\n","        \"\"\" Tests the model with the highest validation loss on the test iterator\n","        \"\"\"\n","        # self.model.load_state_dict(torch.load(f'{self.model_name}.pt'))\n","        torch.load(f'{self.model_name}.pt')\n","        test_loss, test_acc, f1, ece = self.evaluate_epoch(self.test_iter)\n","        print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%\\n | F1 Score: {f1:.3f} | Calibration Error: {ece:.3f} |')\n","        with open('model_results.txt', \"a\") as f:\n","            f.write(f'{self.model_name}\\nTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%\\n | F1 Score: {f1:.3f} | Calibration Error: {ece:.3f} |'  )\n","\n","    def count_parameters(self):\n","        num_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n","        print(f'The model has {num_params:,} trainable parameters')\n","\n","    def train_epoch(self):\n","        \"\"\" Trains the model for 1 epoch of the train iterator\n","\n","        Returns:\n","            tuple: (loss, accuracy)\n","        \"\"\"\n","        epoch_loss = 0\n","        epoch_acc = 0\n","        self.model.train()\n","        for batch in self.train_iter:\n","            # print(batch.__dict__)\n","            self.optimizer.zero_grad()\n","            predictions = self.model(getattr(batch, batch.input_fields[0]))\n","            # Reduce dimentionality if using binary cross entropy loss\n","            if self.criterion_str == \"BCE\":\n","                predictions = predictions.squeeze(1)\n","                acc_formula = binary_accuracy\n","            elif self.criterion_str == \"CE\":\n","                acc_formula = categorical_accuracy\n","            loss = self.criterion(predictions, getattr(batch, batch.target_fields[0]))\n","            acc = acc_formula(predictions, getattr(batch, batch.target_fields[0]))\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","        return epoch_loss / len(self.train_iter), epoch_acc / len(self.train_iter)\n","\n","    def evaluate_epoch(self, iterator):\n","        epoch_loss = 0\n","        epoch_acc = 0\n","        logits_list = []\n","        labels_list = []\n","        self.model.eval()\n","        with torch.no_grad():\n","            for batch in iterator:\n","                predictions = self.model(getattr(batch, batch.input_fields[0]))\n","                logits_list.append(predictions)\n","                labels_list.append(getattr(batch, batch.target_fields[0]))\n","                # Reduce dimentionality if using binary cross entropy loss\n","                if self.criterion_str == \"BCE\":\n","                    predictions = predictions.squeeze(1)\n","                    acc_formula = binary_accuracy\n","                elif self.criterion_str == \"CE\":\n","                    acc_formula = categorical_accuracy\n","                loss = self.criterion(predictions, getattr(batch, batch.target_fields[0]))\n","                acc = acc_formula(predictions, getattr(batch, batch.target_fields[0]))\n","\n","                epoch_loss += loss.item()\n","                epoch_acc += acc.item()\n","        logits_list = torch.cat(logits_list)\n","        labels_list = torch.cat(labels_list)\n","        # softmaxes = F.softmax(logits_list, dim=1)\n","        # _, predictions_list = torch.max(softmaxes, dim=1)\n","        predictions_list = torch.round(torch.sigmoid(logits_list))\n","        \n","        #create ece class object\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        ece_criterion = ECE().to(device)\n","\n","        # first input is raw logits tensor (not softmaxed) and the second is a tensor of correct labels \n","        ece_test = ece_criterion(logits_list, labels_list).item()\n","        \n","        f1 = f1_score(labels_list.detach().cpu().numpy(),predictions_list.detach().cpu().numpy(), average='weighted')\n","        return epoch_loss / len(iterator), epoch_acc / len(iterator), f1, ece_test\n","      \n","    def do_what_we_want(self, num_epochs=10):\n","        self.load_dataset()\n","        self.build_vocab()\n","        self.init_model()\n","        self.init_optimizer()\n","        self.train_model(num_epochs)\n","        self.test_model()\n","\n","\n","def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    # round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float()  # convert into float for division\n","    acc = correct.sum() / len(correct)\n","    return acc\n","\n","\n","def categorical_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    top_pred = preds.argmax(1, keepdim = True)\n","    correct = top_pred.eq(y.view_as(top_pred)).sum()\n","    acc = correct.float() / y.shape[0]\n","    return acc\n","  \n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-hsTbqAgqy9y","executionInfo":{"status":"ok","timestamp":1617757373616,"user_tz":240,"elapsed":351284,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}},"outputId":"2893c1bb-bb60-4322-e732-4cf880b4c2e0"},"source":["import gc\n","\n","if __name__ == \"__main__\":\n","    # clear contents of file\n","    with open(\"model_results.txt\", \"w\") as f:\n","        pass\n","    \n","\n","    # ======================== FORMALITY BINARY MODEL ========================= #\n","    trainer = ModelTrainer(\n","        model_name=\"CNN-formality-binary\", \n","        text_field_name='Sentence', \n","        label_field_name='Formality', \n","        train_file_name='mturk_news_formality_BINARY.csv', \n","        criterion_str='BCE',\n","        dtype=torch.float)\n","    trainer.do_what_we_want(num_epochs=8)\n","\n","    \n","    # ======================== INFORMATIVENESS BINARY MODEL ========================= #\n","    trainer = ModelTrainer(\n","        model_name=\"CNN-informativeness-binary\", \n","        text_field_name='Sentence', \n","        label_field_name='Informativeness', \n","        train_file_name='mturk_news_informativeness_BINARY.csv', \n","        criterion_str='BCE',\n","        dtype=torch.float)\n","    trainer.do_what_we_want(num_epochs=8)\n","\n","\n","    # ======================== EMOTION MODEL ========================= #\n","    # trainer = ModelTrainer(\n","    #     model_name=\"CNN-emotion\", \n","    #     text_field_name='Text', \n","    #     label_field_name='Emotion', \n","    #     train_file_name='emotion_train.csv', \n","    #     test_file_name='emotion_test.csv',\n","    #     criterion_str='CE'\n","    # )\n","    # trainer.do_what_we_want(num_epochs=8)\n","        # --------------- deep archetecture -------------------#\n","    # trainer = ModelTrainer(\n","    #     model_name=\"deep-CNN-emotion\", \n","    #     model_type='deep', \n","    #     text_field_name='Text', \n","    #     label_field_name='Emotion', \n","    #     train_file_name='/content/emotion_train.csv', \n","    #     test_file_name='/content/emotion_test.csv'\n","    # )\n","    # trainer.do_what_we_want(num_epochs=15)\n","\n","\n","    # ======================== SENTIMENT MODEL ========================= #\n","    trainer = ModelTrainer(\"CNN-sentiment\", IMDB=True, criterion_str=\"BCE\", dtype=torch.float)\n","    trainer.do_what_we_want(num_epochs=8)\n","\n","\n","    # ======================== SARCASM BASE MODEL ========================= #\n","    trainer = ModelTrainer(\n","        model_name=\"CNN-sarcasm-base\", \n","        text_field_name='text', \n","        label_field_name='label', \n","        train_file_name='sarcasm-hard.csv', \n","        criterion_str='BCE',\n","        dtype=torch.float)\n","    trainer.do_what_we_want(num_epochs=8)\n","\n","\n","    # ======================== SARCASM MASTER MODEL ========================= #\n","    # trainer = ModelTrainer(\n","    #     model_name=\"CNN-sarcasm-master\",\n","    #     model_type=\"master\",\n","    #     text_field_name='headline', \n","    #     label_field_name='is_sarcastic', \n","    #     train_file_name='Sarcasm_Headlines_Dataset_v2.csv', \n","    #     criterion_str='BCE',\n","    #     dtype=torch.float)\n","    # trainer.do_what_we_want(num_epochs=8)\n","\n","\n","    # ======================== SARCASM MASTER MODEL ========================= #\n","    trainer = ModelTrainer(\n","        model_name=\"CNN-sarcasm-master-without-base\",\n","        model_type=\"master\",\n","        text_field_name='headline', \n","        label_field_name='is_sarcastic', \n","        train_file_name='Sarcasm_Headlines_Dataset_v2.csv', \n","        criterion_str='BCE',\n","        dtype=torch.float,\n","        use_base=False)\n","    trainer.do_what_we_want(num_epochs=8)\n","\n","\n","\n","    # ======================== SARCASM BASE MODEL ========================= #\n","    # trainer = ModelTrainer(\n","    #     model_name=\"CNN-sarcasm-base\", \n","    #     text_field_name='headline', \n","    #     label_field_name='is_sarcastic', \n","    #     train_file_name='Sarcasm_Headlines_Dataset_v2.csv', \n","    #     criterion_str='BCE',\n","    #     dtype=torch.float)\n","    # trainer.do_what_we_want(num_epochs=8)\n","\n","\n","    # emotion_trainer = ModelTrainer(\n","    #     model_name=\"CNN-emotion\", \n","    #     text_field_name='Text', \n","    #     label_field_name='Emotion', \n","    #     train_file_name='emotion_train.csv', \n","    #     test_file_name='emotion_test.csv'\n","    # )\n","    # emotion_trainer.do_what_we_want(num_epochs=8)\n","    # # Clear GPU memory\n","    # emotion_trainer = None\n","    # gc.collect()\n","    # torch.cuda.empty_cache()\n","\n","    # deep_sentament_trainer = ModelTrainer(\"deep-CNN-sentiment\", model_type='deep', IMDB=True, criterion_str=\"BCE\")\n","    # deep_sentament_trainer.do_what_we_want(num_epochs=8)\n","    # # Clear GPU memory\n","    # deep_sentament_trainer = None\n","    # gc.collect()\n","    # torch.cuda.empty_cache()"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Tesla T4\n","CNN-formality-binary\n","Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.356 | Train Acc: 86.66%\n","\t Val. Loss: 0.281 |  Val. Acc: 90.85%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.267 | Train Acc: 89.86%\n","\t Val. Loss: 0.260 |  Val. Acc: 90.85%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.230 | Train Acc: 90.11%\n","\t Val. Loss: 0.252 |  Val. Acc: 91.18%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.194 | Train Acc: 90.82%\n","\t Val. Loss: 0.247 |  Val. Acc: 91.41%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.158 | Train Acc: 93.15%\n","\t Val. Loss: 0.244 |  Val. Acc: 91.74%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.108 | Train Acc: 96.46%\n","\t Val. Loss: 0.266 |  Val. Acc: 91.63%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.070 | Train Acc: 97.59%\n","\t Val. Loss: 0.318 |  Val. Acc: 91.85%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.045 | Train Acc: 98.78%\n","\t Val. Loss: 0.350 |  Val. Acc: 91.85%\n","Test Loss: 0.342 | Test Acc: 92.91%\n"," | F1 Score: 0.914 | Calibration Error: 0.075 |\n","Tesla T4\n","CNN-informativeness-binary\n","Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.319 | Train Acc: 89.25%\n","\t Val. Loss: 0.177 |  Val. Acc: 94.42%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.209 | Train Acc: 93.32%\n","\t Val. Loss: 0.166 |  Val. Acc: 94.42%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.184 | Train Acc: 93.15%\n","\t Val. Loss: 0.161 |  Val. Acc: 94.42%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.174 | Train Acc: 92.98%\n","\t Val. Loss: 0.155 |  Val. Acc: 94.42%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.147 | Train Acc: 93.15%\n","\t Val. Loss: 0.153 |  Val. Acc: 94.42%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.123 | Train Acc: 93.24%\n","\t Val. Loss: 0.148 |  Val. Acc: 94.98%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.110 | Train Acc: 95.25%\n","\t Val. Loss: 0.160 |  Val. Acc: 94.64%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.069 | Train Acc: 97.83%\n","\t Val. Loss: 0.177 |  Val. Acc: 94.87%\n","Test Loss: 0.196 | Test Acc: 95.07%\n"," | F1 Score: 0.935 | Calibration Error: 0.056 |\n","Tesla T4\n","CNN-sentiment\n","Epoch: 01 | Epoch Time: 0m 24s\n","\tTrain Loss: 0.661 | Train Acc: 59.41%\n","\t Val. Loss: 0.488 |  Val. Acc: 78.07%\n","Epoch: 02 | Epoch Time: 0m 24s\n","\tTrain Loss: 0.428 | Train Acc: 80.31%\n","\t Val. Loss: 0.348 |  Val. Acc: 84.75%\n","Epoch: 03 | Epoch Time: 0m 24s\n","\tTrain Loss: 0.300 | Train Acc: 87.27%\n","\t Val. Loss: 0.327 |  Val. Acc: 85.87%\n","Epoch: 04 | Epoch Time: 0m 23s\n","\tTrain Loss: 0.217 | Train Acc: 91.21%\n","\t Val. Loss: 0.332 |  Val. Acc: 86.41%\n","Epoch: 05 | Epoch Time: 0m 23s\n","\tTrain Loss: 0.160 | Train Acc: 93.89%\n","\t Val. Loss: 0.360 |  Val. Acc: 85.96%\n","Epoch: 06 | Epoch Time: 0m 23s\n","\tTrain Loss: 0.116 | Train Acc: 95.69%\n","\t Val. Loss: 0.415 |  Val. Acc: 85.68%\n","Epoch: 07 | Epoch Time: 0m 23s\n","\tTrain Loss: 0.090 | Train Acc: 96.57%\n","\t Val. Loss: 0.450 |  Val. Acc: 85.74%\n","Epoch: 08 | Epoch Time: 0m 23s\n","\tTrain Loss: 0.072 | Train Acc: 97.41%\n","\t Val. Loss: 0.479 |  Val. Acc: 85.94%\n","Test Loss: 0.550 | Test Acc: 84.11%\n"," | F1 Score: 0.841 | Calibration Error: 0.500 |\n","Tesla T4\n","CNN-sarcasm-base\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.313 | Train Acc: 87.34%\n","\t Val. Loss: 0.268 |  Val. Acc: 89.49%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.222 | Train Acc: 91.93%\n","\t Val. Loss: 0.224 |  Val. Acc: 91.36%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.155 | Train Acc: 94.44%\n","\t Val. Loss: 0.238 |  Val. Acc: 91.32%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.115 | Train Acc: 95.88%\n","\t Val. Loss: 0.229 |  Val. Acc: 92.05%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.082 | Train Acc: 97.24%\n","\t Val. Loss: 0.262 |  Val. Acc: 92.19%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.065 | Train Acc: 97.65%\n","\t Val. Loss: 0.274 |  Val. Acc: 91.86%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.042 | Train Acc: 98.62%\n","\t Val. Loss: 0.302 |  Val. Acc: 91.57%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.041 | Train Acc: 98.52%\n","\t Val. Loss: 0.387 |  Val. Acc: 91.45%\n","Test Loss: 0.412 | Test Acc: 90.78%\n"," | F1 Score: 0.909 | Calibration Error: 0.367 |\n","Tesla T4\n","CNN-sarcasm-master-without-base\n","Epoch: 01 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.496 | Train Acc: 75.20%\n","\t Val. Loss: 0.371 |  Val. Acc: 83.92%\n","Epoch: 02 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.240 | Train Acc: 90.39%\n","\t Val. Loss: 0.335 |  Val. Acc: 86.17%\n","Epoch: 03 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.080 | Train Acc: 97.32%\n","\t Val. Loss: 0.385 |  Val. Acc: 86.60%\n","Epoch: 04 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.014 | Train Acc: 99.77%\n","\t Val. Loss: 0.520 |  Val. Acc: 85.90%\n","Epoch: 05 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.004 | Train Acc: 99.98%\n","\t Val. Loss: 0.528 |  Val. Acc: 87.03%\n","Epoch: 06 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.001 | Train Acc: 100.00%\n","\t Val. Loss: 0.548 |  Val. Acc: 86.99%\n","Epoch: 07 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.000 | Train Acc: 100.00%\n","\t Val. Loss: 0.568 |  Val. Acc: 87.09%\n","Epoch: 08 | Epoch Time: 0m 4s\n","\tTrain Loss: 0.000 | Train Acc: 100.00%\n","\t Val. Loss: 0.584 |  Val. Acc: 87.09%\n","Test Loss: 0.565 | Test Acc: 86.97%\n"," | F1 Score: 0.869 | Calibration Error: 0.474 |\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2BbpCHDgl1D","outputId":"0f99bf28-e9be-46cb-8d63-e5cb1f2f8a92"},"source":["DUAL_FC_LAYERS = True\n","\n","# ======================== FORMALITY BINARY MODEL ========================= #\n","trainer = ModelTrainer(\n","    model_name=\"CNN-formality-binary\", \n","    text_field_name='Sentence', \n","    label_field_name='Formality', \n","    train_file_name='mturk_news_formality_BINARY.csv', \n","    criterion_str='BCE',\n","    dtype=torch.float)\n","trainer.do_what_we_want(num_epochs=8)\n","\n","\n","# ======================== INFORMATIVENESS BINARY MODEL ========================= #\n","trainer = ModelTrainer(\n","    model_name=\"CNN-informativeness-binary\", \n","    text_field_name='Sentence', \n","    label_field_name='Informativeness', \n","    train_file_name='mturk_news_informativeness_BINARY.csv', \n","    criterion_str='BCE',\n","    dtype=torch.float)\n","trainer.do_what_we_want(num_epochs=8)\n","\n","\n","# ======================== EMOTION MODEL ========================= #\n","trainer = ModelTrainer(\n","    model_name=\"CNN-emotion\", \n","    text_field_name='Text', \n","    label_field_name='Emotion', \n","    train_file_name='emotion_train.csv', \n","    test_file_name='emotion_test.csv',\n","    criterion_str='CE'\n",")\n","trainer.do_what_we_want(num_epochs=8)\n","    # --------------- deep archetecture -------------------#\n","# trainer = ModelTrainer(\n","#     model_name=\"deep-CNN-emotion\", \n","#     model_type='deep', \n","#     text_field_name='Text', \n","#     label_field_name='Emotion', \n","#     train_file_name='/content/emotion_train.csv', \n","#     test_file_name='/content/emotion_test.csv'\n","# )\n","# trainer.do_what_we_want(num_epochs=15)\n","\n","\n","# ======================== SENTIMENT MODEL ========================= #\n","trainer = ModelTrainer(\"CNN-sentiment\", IMDB=True, criterion_str=\"BCE\", dtype=torch.float)\n","trainer.do_what_we_want(num_epochs=8)\n","\n","\n","# ======================== SARCASM BASE MODEL ========================= #\n","trainer = ModelTrainer(\n","    model_name=\"CNN-sarcasm-base\", \n","    text_field_name='text', \n","    label_field_name='label', \n","    train_file_name='sarcasm-hard.csv', \n","    criterion_str='BCE',\n","    dtype=torch.float)\n","trainer.do_what_we_want(num_epochs=8)\n","\n","\n","# ======================== SARCASM MASTER MODEL ========================= #\n","trainer = ModelTrainer(\n","    model_name=\"CNN-sarcasm-master\",\n","    model_type=\"master\",\n","    text_field_name='headline', \n","    label_field_name='is_sarcastic', \n","    train_file_name='Sarcasm_Headlines_Dataset_v2.csv', \n","    criterion_str='BCE',\n","    dtype=torch.float)\n","trainer.do_what_we_want(num_epochs=8)\n","\n","\n","# ======================== SARCASM MASTER MODEL ========================= #\n","trainer = ModelTrainer(\n","    model_name=\"CNN-sarcasm-master-with-base\",\n","    model_type=\"master\",\n","    text_field_name='headline', \n","    label_field_name='is_sarcastic', \n","    train_file_name='Sarcasm_Headlines_Dataset_v2.csv', \n","    criterion_str='BCE',\n","    dtype=torch.float,\n","    use_base=True)\n","trainer.do_what_we_want(num_epochs=8)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tesla P100-PCIE-16GB\n","CNN-formality-binary\n","Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.356 | Train Acc: 86.66%\n","\t Val. Loss: 0.281 |  Val. Acc: 90.85%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.267 | Train Acc: 89.86%\n","\t Val. Loss: 0.260 |  Val. Acc: 90.85%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.230 | Train Acc: 90.11%\n","\t Val. Loss: 0.252 |  Val. Acc: 91.18%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.194 | Train Acc: 90.82%\n","\t Val. Loss: 0.247 |  Val. Acc: 91.41%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.158 | Train Acc: 93.15%\n","\t Val. Loss: 0.244 |  Val. Acc: 91.74%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.108 | Train Acc: 96.46%\n","\t Val. Loss: 0.266 |  Val. Acc: 91.63%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.070 | Train Acc: 97.59%\n","\t Val. Loss: 0.318 |  Val. Acc: 91.85%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.045 | Train Acc: 98.78%\n","\t Val. Loss: 0.350 |  Val. Acc: 91.85%\n","Test Loss: 0.342 | Test Acc: 92.91%\n","Tesla P100-PCIE-16GB\n","CNN-informativeness-binary\n","Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.319 | Train Acc: 89.25%\n","\t Val. Loss: 0.177 |  Val. Acc: 94.42%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.209 | Train Acc: 93.32%\n","\t Val. Loss: 0.166 |  Val. Acc: 94.42%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.184 | Train Acc: 93.15%\n","\t Val. Loss: 0.161 |  Val. Acc: 94.42%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.174 | Train Acc: 92.98%\n","\t Val. Loss: 0.155 |  Val. Acc: 94.42%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.147 | Train Acc: 93.15%\n","\t Val. Loss: 0.153 |  Val. Acc: 94.42%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.123 | Train Acc: 93.24%\n","\t Val. Loss: 0.148 |  Val. Acc: 94.98%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.110 | Train Acc: 95.25%\n","\t Val. Loss: 0.160 |  Val. Acc: 94.64%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.069 | Train Acc: 97.83%\n","\t Val. Loss: 0.177 |  Val. Acc: 94.87%\n","Test Loss: 0.196 | Test Acc: 95.07%\n","Tesla P100-PCIE-16GB\n","CNN-emotion\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.359 | Train Acc: 39.50%\n","\t Val. Loss: 1.186 |  Val. Acc: 49.95%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.086 | Train Acc: 55.64%\n","\t Val. Loss: 0.979 |  Val. Acc: 62.71%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.799 | Train Acc: 69.92%\n","\t Val. Loss: 0.851 |  Val. Acc: 68.39%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.634 | Train Acc: 77.45%\n","\t Val. Loss: 0.815 |  Val. Acc: 70.23%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.488 | Train Acc: 83.19%\n","\t Val. Loss: 0.848 |  Val. Acc: 70.04%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.413 | Train Acc: 85.59%\n","\t Val. Loss: 0.840 |  Val. Acc: 70.86%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.332 | Train Acc: 89.12%\n","\t Val. Loss: 0.969 |  Val. Acc: 69.90%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.275 | Train Acc: 90.67%\n","\t Val. Loss: 0.919 |  Val. Acc: 71.61%\n","Test Loss: 0.873 | Test Acc: 73.38%\n","Tesla P100-PCIE-16GB\n","CNN-sentiment\n","Epoch: 01 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.659 | Train Acc: 59.72%\n","\t Val. Loss: 0.498 |  Val. Acc: 77.23%\n","Epoch: 02 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.431 | Train Acc: 79.96%\n","\t Val. Loss: 0.351 |  Val. Acc: 84.74%\n","Epoch: 03 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.297 | Train Acc: 87.53%\n","\t Val. Loss: 0.351 |  Val. Acc: 84.99%\n","Epoch: 04 | Epoch Time: 0m 20s\n","\tTrain Loss: 0.219 | Train Acc: 91.10%\n","\t Val. Loss: 0.327 |  Val. Acc: 86.38%\n","Epoch: 05 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.155 | Train Acc: 93.92%\n","\t Val. Loss: 0.361 |  Val. Acc: 86.29%\n","Epoch: 06 | Epoch Time: 0m 20s\n","\tTrain Loss: 0.112 | Train Acc: 95.95%\n","\t Val. Loss: 0.422 |  Val. Acc: 86.06%\n","Epoch: 07 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.088 | Train Acc: 96.88%\n","\t Val. Loss: 0.493 |  Val. Acc: 86.03%\n","Epoch: 08 | Epoch Time: 0m 21s\n","\tTrain Loss: 0.073 | Train Acc: 97.45%\n","\t Val. Loss: 0.538 |  Val. Acc: 86.21%\n","Test Loss: 0.597 | Test Acc: 84.45%\n","Tesla P100-PCIE-16GB\n","CNN-sarcasm-base\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.313 | Train Acc: 87.36%\n","\t Val. Loss: 0.268 |  Val. Acc: 89.52%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.222 | Train Acc: 91.92%\n","\t Val. Loss: 0.224 |  Val. Acc: 91.40%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.155 | Train Acc: 94.44%\n","\t Val. Loss: 0.239 |  Val. Acc: 91.32%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.115 | Train Acc: 95.91%\n","\t Val. Loss: 0.228 |  Val. Acc: 91.92%\n","Epoch: 05 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.080 | Train Acc: 97.33%\n","\t Val. Loss: 0.266 |  Val. Acc: 92.12%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.065 | Train Acc: 97.74%\n","\t Val. Loss: 0.279 |  Val. Acc: 91.97%\n","Epoch: 07 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.045 | Train Acc: 98.61%\n","\t Val. Loss: 0.295 |  Val. Acc: 91.44%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.039 | Train Acc: 98.49%\n","\t Val. Loss: 0.375 |  Val. Acc: 91.62%\n","Test Loss: 0.398 | Test Acc: 90.94%\n","Tesla P100-PCIE-16GB\n","CNN-sarcasm-master\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.496 | Train Acc: 75.17%\n","\t Val. Loss: 0.370 |  Val. Acc: 83.97%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.239 | Train Acc: 90.58%\n","\t Val. Loss: 0.339 |  Val. Acc: 85.90%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.080 | Train Acc: 97.42%\n","\t Val. Loss: 0.391 |  Val. Acc: 86.73%\n","Epoch: 04 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.013 | Train Acc: 99.79%\n","\t Val. Loss: 0.485 |  Val. Acc: 86.86%\n","Epoch: 05 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.002 | Train Acc: 99.99%\n","\t Val. Loss: 0.513 |  Val. Acc: 87.05%\n","Epoch: 06 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.001 | Train Acc: 100.00%\n","\t Val. Loss: 0.540 |  Val. Acc: 86.97%\n","Epoch: 07 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.000 | Train Acc: 100.00%\n","\t Val. Loss: 0.560 |  Val. Acc: 87.14%\n","Epoch: 08 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.000 | Train Acc: 100.00%\n","\t Val. Loss: 0.575 |  Val. Acc: 86.99%\n","Test Loss: 0.558 | Test Acc: 87.07%\n","Tesla P100-PCIE-16GB\n","CNN-sarcasm-master-with-base\n","Epoch: 01 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.661 | Train Acc: 60.18%\n","\t Val. Loss: 0.618 |  Val. Acc: 66.73%\n","Epoch: 02 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.541 | Train Acc: 73.06%\n","\t Val. Loss: 0.534 |  Val. Acc: 73.73%\n","Epoch: 03 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.399 | Train Acc: 82.86%\n","\t Val. Loss: 0.491 |  Val. Acc: 77.29%\n","Epoch: 04 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.285 | Train Acc: 88.62%\n","\t Val. Loss: 0.486 |  Val. Acc: 78.99%\n","Epoch: 05 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.201 | Train Acc: 92.93%\n","\t Val. Loss: 0.497 |  Val. Acc: 79.32%\n","Epoch: 06 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.137 | Train Acc: 95.68%\n","\t Val. Loss: 0.530 |  Val. Acc: 79.57%\n","Epoch: 07 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.096 | Train Acc: 97.17%\n","\t Val. Loss: 0.562 |  Val. Acc: 79.60%\n","Epoch: 08 | Epoch Time: 0m 5s\n","\tTrain Loss: 0.067 | Train Acc: 98.25%\n","\t Val. Loss: 0.591 |  Val. Acc: 79.56%\n","Test Loss: 0.602 | Test Acc: 79.34%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R9OOcC_7ipgT"},"source":[""],"execution_count":null,"outputs":[]}]}