{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-CNNs-Emotion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMZ-spFWQ-rK"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class CNN(nn.Module):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \r\n",
        "                 dropout, pad_idx):\r\n",
        "        super().__init__()\r\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\r\n",
        "        self.convs = nn.ModuleList([\r\n",
        "                                    nn.Conv2d(in_channels = 1, \r\n",
        "                                              out_channels = n_filters, \r\n",
        "                                              kernel_size = (fs, embedding_dim)) \r\n",
        "                                    for fs in filter_sizes\r\n",
        "                                    ])\r\n",
        "        # =========== OLD APPROACH FOR FIXED NUM OF DIFFERENT FILTERS =========== #\r\n",
        "        # self.conv_0 = nn.Conv2d(in_channels = 1, \r\n",
        "        #                         out_channels = n_filters, \r\n",
        "        #                         kernel_size = (filter_sizes[0], embedding_dim))\r\n",
        "        \r\n",
        "        # self.conv_1 = nn.Conv2d(in_channels = 1, \r\n",
        "        #                         out_channels = n_filters, \r\n",
        "        #                         kernel_size = (filter_sizes[1], embedding_dim))\r\n",
        "        \r\n",
        "        # self.conv_2 = nn.Conv2d(in_channels = 1, \r\n",
        "        #                         out_channels = n_filters, \r\n",
        "        #                         kernel_size = (filter_sizes[2], embedding_dim))\r\n",
        "        \r\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, text):\r\n",
        "        #text = [batch size, sent len]\r\n",
        "        embedded = self.embedding(text)\r\n",
        "                \r\n",
        "        #embedded = [batch size, sent len, emb dim]\r\n",
        "        embedded = embedded.unsqueeze(1)\r\n",
        "        \r\n",
        "        #embedded = [batch size, 1, sent len, emb dim]\r\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\r\n",
        "            \r\n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\r\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\r\n",
        "\r\n",
        "        # =========== OLD APPROACH FOR FIXED NUM OF DIFFERENT FILTERS =========== #\r\n",
        "        # #embedded = [batch size, 1, sent len, emb dim]\r\n",
        "        \r\n",
        "        # conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\r\n",
        "        # conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\r\n",
        "        # conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\r\n",
        "            \r\n",
        "        # #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\r\n",
        "        \r\n",
        "        # pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\r\n",
        "        # pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\r\n",
        "        # pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\r\n",
        "        \r\n",
        "        #pooled_n = [batch size, n_filters]\r\n",
        "        # cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))\r\n",
        "\r\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\r\n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\r\n",
        "        \r\n",
        "        return self.fc(cat)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARVjzpD41Suy"
      },
      "source": [
        "import torch\r\n",
        "from torchtext.legacy import data\r\n",
        "from torchtext.legacy import datasets\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "SEED = 1234\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "\r\n",
        "# create fields in emotion dataset\r\n",
        "TEXT = data.Field(tokenize = 'spacy', \r\n",
        "                  tokenizer_language = 'en_core_web_sm',\r\n",
        "                  batch_first = True)\r\n",
        "EMOTION = data.Field()\r\n",
        "\r\n",
        "# tuples representative of tabular format\r\n",
        "fields = [\r\n",
        "          ('Emotion', EMOTION),\r\n",
        "          ('Text', TEXT)\r\n",
        "]\r\n",
        "\r\n",
        "# LOAD EMOTION DATA\r\n",
        "em_train, em_test = data.TabularDataset.splits(\r\n",
        "    path='/content',\r\n",
        "    train='emotion_train.csv',\r\n",
        "    test='emotion_test.csv',\r\n",
        "    format='csv',\r\n",
        "    fields=fields,\r\n",
        "    skip_header=False\r\n",
        ")\r\n",
        "\r\n",
        "em_train, em_valid = em_train.split(random_state = random.seed(SEED))\r\n",
        "\r\n",
        "# USED FOR SENTIMENT ANALYSIS\r\n",
        "\r\n",
        "# np.random.seed(SEED)\r\n",
        "# torch.manual_seed(SEED)\r\n",
        "# torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "# TEXT = data.Field(tokenize = 'spacy', \r\n",
        "#                   tokenizer_language = 'en_core_web_sm',\r\n",
        "#                   batch_first = True)\r\n",
        "# LABEL = data.LabelField(dtype = torch.float)\r\n",
        "\r\n",
        "# train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\r\n",
        "\r\n",
        "# train_data, valid_data = train_data.split(random_state = random.seed(SEED))\r\n",
        "\r\n",
        "# em_train = pd.read_csv('/content/emotion_train.csv')\r\n",
        "# em_test = pd.read_csv('/content/emotion_test.csv')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "a-MWdLiqRL02",
        "outputId": "0881faed-1a98-4360-c77c-d36c62efb4cb"
      },
      "source": [
        "# MAX_VOCAB_SIZE = 25_000\r\n",
        "\r\n",
        "TEXT.build_vocab(em_train, \r\n",
        "                 max_size = MAX_VOCAB_SIZE, \r\n",
        "                 vectors = \"glove.6B.100d\", \r\n",
        "                 unk_init = torch.Tensor.normal_\r\n",
        "                 )\r\n",
        "\r\n",
        "# EMOTION.build_vocab(em_train)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b290cc825594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0mmax_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_VOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove.6B.100d\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                  \u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                  )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0msources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/legacy/data/dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Example' object has no attribute 'Text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FdMerp42kgP"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "em_train_iterator, em_valid_iterator, em_test_iterator = data.BucketIterator.splits(\r\n",
        "    (em_train, em_valid, em_test), \r\n",
        "    batch_size = BATCH_SIZE, \r\n",
        "    device = device)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "XSoKv9Ps2_pw",
        "outputId": "7ec327c9-bed8-44f9-baa2-35127a1beaac"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\r\n",
        "EMBEDDING_DIM = 100\r\n",
        "N_FILTERS = 100\r\n",
        "FILTER_SIZES = [3,4,5]\r\n",
        "OUTPUT_DIM = 1\r\n",
        "DROPOUT = 0.5\r\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\r\n",
        "\r\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-6a49685a6616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mINPUT_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mN_FILTERS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mFILTER_SIZES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mOUTPUT_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Field' object has no attribute 'vocab'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAGDEdm23EFP"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK_9LWtD3FjU"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\r\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\r\n",
        "\r\n",
        "# zero initial weights of <unk> and <pad>\r\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\r\n",
        "\r\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\r\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}