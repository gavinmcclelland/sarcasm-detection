{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"8_CNN_Master.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('825': venv)","metadata":{"interpreter":{"hash":"59650d09dba9c4f7c5c1641a7604d868492dda203335bf6b3bcf6f068be5d520"}}},"language_info":{"name":"python","version":"3.8.8-final"}},"cells":[{"cell_type":"code","metadata":{"id":"et2PV0nl1_Vs","executionInfo":{"status":"ok","timestamp":1617584115214,"user_tz":240,"elapsed":921,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","\n","import torch\n","\n","# torch.cuda.empty_cache()"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zz-pw0mQ7rrM","executionInfo":{"status":"ok","timestamp":1617584116901,"user_tz":240,"elapsed":252,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["class Identity(torch.nn.Module):\n","    def __init__(self):\n","        super(Identity, self).__init__()\n","\n","    def forward(self, x):\n","        return x"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"EYIA9hGCs1Ds","executionInfo":{"status":"ok","timestamp":1617584118094,"user_tz":240,"elapsed":278,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","class CNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes,\n","                 output_dim, dropout, pad_idx):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","        self.convs = nn.ModuleList([\n","                                    nn.Conv2d(in_channels=1,\n","                                              out_channels=n_filters, \n","                                              kernel_size=(fs, embedding_dim))\n","                                    for fs in filter_sizes\n","                                    ])\n","        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","        # self.fc = Identity()\n","        # self.dropout = Identity()\n","\n","    def forward(self, text):\n","        # text = [batch size, sent len]\n","        embedded = self.embedding(text)\n","\n","        # embedded = [batch size, sent len, emb dim]\n","        embedded = embedded.unsqueeze(1)\n","\n","        # embedded = [batch size, 1, sent len, emb dim]\n","        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n","\n","        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n","        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n","\n","        cat = self.dropout(torch.cat(pooled, dim=1))\n","\n","        return self.fc(cat)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5bgxBoWOITL","executionInfo":{"status":"ok","timestamp":1617588022017,"user_tz":240,"elapsed":266,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["class SUPER_CNN(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes,\n","                 output_dim, dropout, pad_idx):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","        self.convs = nn.ModuleList([\n","                                    nn.Conv2d(in_channels=1,\n","                                              out_channels=n_filters, \n","                                              kernel_size=(fs, embedding_dim))\n","                                    for fs in filter_sizes\n","                                    ])\n","\n","        # self.sentiment_model = CNN(vocab_size, embedding_dim, n_filters, filter_sizes,\n","        #          output_dim, dropout, pad_idx)\n","        # self.sentiment_model.load_state_dict(torch.load('CNN-sentiment.pt'))\n","        self.sentiment_model = torch.load('CNN-sentiment.pt')\n","        for param in self.sentiment_model.parameters():\n","          param.requires_grad = False\n","        self.sentiment_model.fc = Identity()\n","        self.sentiment_model.dropout = Identity()\n","        \n","        # self.emotion_model = CNN(vocab_size, embedding_dim, n_filters, filter_sizes,\n","        #          5, dropout, pad_idx)\n","        # self.emotion_model.load_state_dict(torch.load('CNN-emotion.pt'))\n","        self.emotion_model = torch.load('CNN-emotion.pt')\n","        for param in self.emotion_model.parameters():\n","          param.requires_grad = False\n","        self.emotion_model.fc = Identity()\n","        self.emotion_model.dropout = Identity()\n","\n","        # self.formality_model = CNN(vocab_size, embedding_dim, n_filters, filter_sizes,\n","        #          output_dim, dropout, pad_idx)\n","        # self.formality_model.load_state_dict(torch.load('CNN-formality-binary.pt'))\n","        self.formality_model = torch.load('CNN-formality-binary.pt')\n","        for param in self.formality_model.parameters():\n","          param.requires_grad = False\n","        self.formality_model.fc = Identity()\n","        self.formality_model.dropout = Identity()\n","\n","        # self.informativeness_model = CNN(vocab_size, embedding_dim, n_filters, filter_sizes,\n","        #          output_dim, dropout, pad_idx)\n","        # self.informativeness_model.load_state_dict(torch.load('CNN-informativeness-binary.pt'))\n","        self.informativeness_model = torch.load('CNN-informativeness-binary.pt')\n","        for param in self.informativeness_model.parameters():\n","          param.requires_grad = False\n","        self.informativeness_model.fc = Identity()\n","        self.informativeness_model.dropout = Identity()\n","        \n","        self.fc = nn.Linear(5 * len(filter_sizes) * n_filters, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, text):\n","        # text = [batch size, sent len]\n","        embedded = self.embedding(text)\n","\n","        # embedded = [batch size, sent len, emb dim]\n","        embedded = embedded.unsqueeze(1)\n","\n","        # embedded = [batch size, 1, sent len, emb dim]\n","        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n","\n","        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n","        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n","        pooled = torch.cat(pooled,dim=1)\n","\n","        # print(\"TESTING\")\n","        o1 = self.sentiment_model(text)\n","        o2 = self.emotion_model(text)\n","        o3 = self.formality_model(text)\n","        o4 = self.informativeness_model(text)\n","        # print(\"TESTING 2\")\n","        # print(pooled.type)\n","        # print(o1.type)\n","        # print(o2.type)\n","        cat = self.dropout(torch.cat((pooled, o1, o2, o3, o4), dim=1))\n","        # print(cat.shape)\n","\n","        return self.fc(cat)"],"execution_count":265,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MR2ivlR0NPj4"},"source":["## Sarcasm Model Definition"]},{"cell_type":"code","metadata":{"id":"_E8uIyuwNWvL","executionInfo":{"status":"ok","timestamp":1617587148379,"user_tz":240,"elapsed":3519,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["import torch\n","from torchtext.legacy import data\n","from torchtext.legacy import datasets\n","import random\n","import numpy as np\n","\n","SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","# create fields in emotion dataset\n","TEXT = data.Field(tokenize = 'spacy', \n","                  tokenizer_language = 'en_core_web_sm',\n","                  batch_first = True\n","                  )\n","LABEL = data.LabelField(dtype=torch.float)\n","\n","# tuples representative of tabular format\n","fields = [\n","          ('is_sarcastic', LABEL),\n","          ('headline', TEXT)\n","]\n","\n","# LOAD SARCASM DATA\n","sarc_train, = data.TabularDataset.splits(\n","    path='/content',\n","    train='Sarcasm_Headlines_Dataset_v2.csv',\n","    format='csv',\n","    fields=fields,\n","    skip_header=True\n",")"],"execution_count":194,"outputs":[]},{"cell_type":"code","metadata":{"id":"5IE01PjnNmcy","executionInfo":{"status":"ok","timestamp":1617588027124,"user_tz":240,"elapsed":774,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["MAX_VOCAB_SIZE = 25_000\n","\n","sarc_train, sarc_valid, sarc_test = sarc_train.split(random_state = random.seed(SEED), split_ratio=[0.4,0.3,0.3])\n","\n","TEXT.build_vocab(sarc_train, \n","                 max_size = MAX_VOCAB_SIZE, \n","                 vectors = \"glove.6B.100d\", \n","                 unk_init = torch.Tensor.normal_)\n","\n","LABEL.build_vocab(sarc_train)"],"execution_count":266,"outputs":[]},{"cell_type":"code","metadata":{"id":"a2WzVgmqaUJ6","executionInfo":{"status":"ok","timestamp":1617588029602,"user_tz":240,"elapsed":280,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["BATCH_SIZE = 64\n","\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device = torch.device('cpu')\n","\n","sarc_train_iterator, sarc_valid_iterator, sarc_test_iterator = data.BucketIterator.splits(\n","    (sarc_train, sarc_valid, sarc_test), \n","    batch_size = BATCH_SIZE, \n","    device = device,\n","    sort=False)"],"execution_count":267,"outputs":[]},{"cell_type":"code","metadata":{"id":"8C-_zlxcN09X","executionInfo":{"status":"ok","timestamp":1617588033711,"user_tz":240,"elapsed":306,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","N_FILTERS = 100\n","FILTER_SIZES = [3,4,5]\n","OUTPUT_DIM = 1\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","\n","model = SUPER_CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"],"execution_count":268,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vH7qr0G_N7UL","executionInfo":{"status":"ok","timestamp":1617588036806,"user_tz":240,"elapsed":263,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}},"outputId":"085ef799-5406-44ac-bf15-b2c5d787be49"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":269,"outputs":[{"output_type":"stream","text":["The model has 160,801 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d09VO35mOBQp","executionInfo":{"status":"ok","timestamp":1617588040654,"user_tz":240,"elapsed":311,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["pretrained_embeddings = TEXT.vocab.vectors\n","model.embedding.weight.data.copy_(pretrained_embeddings)\n","\n","# zero initial weights of <unk> and <pad>\n","UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\n","model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"],"execution_count":270,"outputs":[]},{"cell_type":"code","metadata":{"id":"FNgQ-SAraKAw","executionInfo":{"status":"ok","timestamp":1617588043008,"user_tz":240,"elapsed":263,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters())\n","criterion = nn.BCEWithLogitsLoss()\n","model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":271,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NpVNnFkmark7","executionInfo":{"status":"ok","timestamp":1617588049203,"user_tz":240,"elapsed":3116,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}},"outputId":"541c9605-5ba7-4602-ddc4-b84515c5ebe1"},"source":["N_EPOCHS = 10\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, sarc_train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, sarc_valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'CNN-sarcasm-extreme.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":272,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.735 | Train Acc: 57.45%\n","\t Val. Loss: 0.694 |  Val. Acc: 57.14%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.701 | Train Acc: 40.43%\n","\t Val. Loss: 0.691 |  Val. Acc: 48.57%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.663 | Train Acc: 61.70%\n","\t Val. Loss: 0.690 |  Val. Acc: 54.29%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.617 | Train Acc: 68.09%\n","\t Val. Loss: 0.690 |  Val. Acc: 62.86%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.572 | Train Acc: 78.72%\n","\t Val. Loss: 0.691 |  Val. Acc: 51.43%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.509 | Train Acc: 91.49%\n","\t Val. Loss: 0.694 |  Val. Acc: 48.57%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.488 | Train Acc: 87.23%\n","\t Val. Loss: 0.697 |  Val. Acc: 48.57%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.479 | Train Acc: 87.23%\n","\t Val. Loss: 0.697 |  Val. Acc: 48.57%\n","Epoch: 09 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.460 | Train Acc: 91.49%\n","\t Val. Loss: 0.695 |  Val. Acc: 48.57%\n","Epoch: 10 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.421 | Train Acc: 100.00%\n","\t Val. Loss: 0.692 |  Val. Acc: 48.57%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jFLG8D7GowI9","executionInfo":{"status":"ok","timestamp":1617588055236,"user_tz":240,"elapsed":262,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}},"outputId":"f4ca0d49-3254-4d43-cc83-1fd225cd6c53"},"source":["# TESTING\n","model.load_state_dict(torch.load('CNN-sarcasm-extreme.pt'))\n","\n","test_loss, test_acc = evaluate(model, sarc_test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"execution_count":273,"outputs":[{"output_type":"stream","text":["Test Loss: 0.698 | Test Acc: 48.57%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NJJcliUuace9","executionInfo":{"status":"ok","timestamp":1617584587076,"user_tz":240,"elapsed":275,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","    return acc\n","\n","def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch in iterator:\n","        # print(batch.__dict__)\n","        optimizer.zero_grad()\n","        predictions = model(batch.headline).squeeze(1)\n","        loss = criterion(predictions, batch.is_sarcastic)\n","        acc = binary_accuracy(predictions, batch.is_sarcastic)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for batch in iterator:\n","            predictions = model(batch.headline).squeeze(1)\n","            loss = criterion(predictions, batch.is_sarcastic)\n","            acc = binary_accuracy(predictions, batch.is_sarcastic)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"EbrlT6PwqutK","executionInfo":{"status":"ok","timestamp":1617583486937,"user_tz":240,"elapsed":4641,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}}},"source":["from torchtext.legacy import data\n","from torchtext.legacy import datasets\n","import torch.optim as optim\n","import random\n","import numpy as np\n","import time\n","\n","SEED = 1234\n","\n","\n","class ModelTrainer():\n","\n","    def __init__(self, model_name, model_type=None, text_field_name=None,\n","                 label_field_name=None, IMDB=False, train_file_name=None,\n","                 test_file_name=None, embedding_dim=100, criterion_str=\"CE\",\n","                 max_vocab_size=25000, num_filters=100, filter_sizes=[3, 4, 5],\n","                 dropout=0.5, output_dim=1, batch_size=64, dtype=torch.long):\n","        self.model_name = model_name\n","        self.model_type = model_type\n","        self.text_field_name = text_field_name\n","        self.label_field_name = label_field_name\n","        self.IMDB = IMDB\n","        self.train_file_name = train_file_name\n","        self.test_file_name = test_file_name\n","        self.criterion_str = criterion_str\n","        self.embedding_dim = embedding_dim\n","        self.max_vocab_size = max_vocab_size\n","        self.num_filters = num_filters\n","        self.filter_sizes = filter_sizes\n","        self.dropout = dropout\n","        self.batch_size = batch_size\n","        self.dtype = dtype\n","\n","    def load_dataset(self):\n","        \"\"\" Loads the dataset using torchtext\n","        \"\"\"\n","        r_seed = random.seed(SEED)\n","        np.random.seed(SEED)\n","        torch.manual_seed(SEED)\n","        torch.backends.cudnn.deterministic = True\n","\n","        self.TEXT = data.Field(\n","            tokenize='spacy',\n","            tokenizer_language='en_core_web_sm',\n","            batch_first=True)\n","        self.LABEL = data.LabelField(dtype=self.dtype) # dtype=torch.float)\n","        # LOAD DATASET\n","        if self.IMDB:\n","            self.dataset, test_data = datasets.IMDB.splits(self.TEXT, self.LABEL)\n","            train_data, valid_data = self.dataset.split(random_state=r_seed)\n","        else:\n","            if self.text_field_name == \"Sentence\":\n","                # Formality and Informativeness don't work with dict fields\n","                # tuples representative of tabular format\n","                fields = [\n","                          (self.label_field_name, self.LABEL),\n","                          (self.text_field_name, self.TEXT)\n","                ]\n","                skip_header = True\n","            else:\n","                fields = {\n","                    self.label_field_name: (self.label_field_name, self.LABEL),\n","                    self.text_field_name: (self.text_field_name, self.TEXT)\n","                }\n","                skip_header = False\n","            format = self.train_file_name.split('.')[-1]\n","            if self.test_file_name:\n","                \n","                self.dataset, test_data = data.TabularDataset.splits(\n","                    path='/content',\n","                    train=self.train_file_name,\n","                    test=self.test_file_name,\n","                    format=format,\n","                    fields=fields,\n","                    skip_header=skip_header\n","                )\n","                train_data, valid_data = self.dataset.split(random_state=r_seed)\n","            else:\n","                self.dataset = data.TabularDataset.splits(\n","                    path='/content',\n","                    train=self.train_file_name,\n","                    format=format,\n","                    fields=fields,\n","                    skip_header=skip_header\n","                )[0]\n","                train_data, valid_data, test_data = self.dataset.split(\n","                    split_ratio=[0.4, 0.3, 0.3], random_state=r_seed)\n","\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        print(torch.cuda.get_device_name(0))\n","\n","        self.train_iter, self.val_iter, self.test_iter = data.BucketIterator.splits(\n","            (train_data, valid_data, test_data),\n","            batch_size=self.batch_size,\n","            device=self.device,\n","            sort=False\n","            )\n","\n","    def build_vocab(self):\n","        \"\"\" Builds the vocabulary of the dataset using torchtext\n","        \"\"\"\n","        self.TEXT.build_vocab(\n","            self.dataset,\n","            max_size=self.max_vocab_size,\n","            vectors=\"glove.6B.100d\",\n","            unk_init=torch.Tensor.normal_\n","        )\n","        self.LABEL.build_vocab(self.dataset)\n","\n","    def init_model(self):\n","        \"\"\" Create CNN model with the supplied/derived parameters.\n","            Loads \"glove.6B.100d\" weights into the model's embedding layer\n","        \"\"\"\n","        # Get the size of the vocabulary\n","        vocab_size = len(self.TEXT.vocab)\n","        if self.criterion_str == \"CE\":\n","          output_dim = len(self.LABEL.vocab)\n","        else:\n","          output_dim = 1\n","        pad_idx = self.TEXT.vocab.stoi[self.TEXT.pad_token]\n","        # Initialize model\n","        if self.model_type == 'deep':\n","          self.model = VDCNN(output_dim, vocab_size, pad_idx,\n","                             self.embedding_dim, shortcut=True)\n","        else:\n","          self.model = CNN(vocab_size, self.embedding_dim, self.num_filters,\n","                           self.filter_sizes, output_dim, self.dropout,\n","                           pad_idx)\n","        # Get pretrained weights from vocab\n","        pretrained_embeddings = self.TEXT.vocab.vectors\n","        self.model.embedding.weight.data.copy_(pretrained_embeddings)\n","\n","        # zero initial weights of <unk> and <pad>\n","        unk_index = self.TEXT.vocab.stoi[self.TEXT.unk_token]\n","\n","        self.model.embedding.weight.data[unk_index] = torch.zeros(self.embedding_dim)\n","        self.model.embedding.weight.data[pad_idx] = torch.zeros(self.embedding_dim)\n","\n","    def init_optimizer(self):\n","        \"\"\" Initializes the optimizor (Adam) and the criterion (BCEWithLogitsLoss)\n","        \"\"\"\n","        self.optimizer = optim.Adam(self.model.parameters())\n","        self.model = self.model.to(self.device)\n","        if self.criterion_str == \"CE\":\n","            self.criterion = nn.CrossEntropyLoss()\n","        elif self.criterion_str == \"BCE\":\n","            self.criterion = nn.BCEWithLogitsLoss()\n","        self.criterion = self.criterion.to(self.device)\n","\n","    def train_model(self, num_epochs):\n","        \"\"\" Trains and validates the model (and prints the results) for the\n","            given number of epochs. Saves the model from the epoch which\n","            yeilded the lowest validation loss.\n","\n","        Args:\n","            num_epochs (int): number of epochs to train the model\n","        \"\"\"\n","        print(self.model_name)\n","        best_valid_loss = float('inf')\n","\n","        for epoch in range(num_epochs):\n","            start_time = time.time()\n","\n","            train_loss, train_acc = self.train_epoch()\n","            valid_loss, valid_acc = self.evaluate_epoch(self.val_iter)\n","\n","            end_time = time.time()\n","\n","            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","            if valid_loss < best_valid_loss:\n","                best_valid_loss = valid_loss\n","                # torch.save(self.model.state_dict(), f'{self.model_name}.pt')\n","                torch.save(self.model, f'{self.model_name}.pt')\n","\n","            print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","\n","    def test_model(self):\n","        \"\"\" Tests the model with the highest validation loss on the test iterator\n","        \"\"\"\n","        # self.model.load_state_dict(torch.load(f'{self.model_name}.pt'))\n","        torch.load(f'{self.model_name}.pt')\n","        test_loss, test_acc = self.evaluate_epoch(self.test_iter)\n","        print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n","        with open('model_results.txt', \"a\") as f:\n","            f.write(f'{self.model_name}\\nTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%\\n')\n","\n","    def count_parameters(self):\n","        num_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n","        print(f'The model has {num_params:,} trainable parameters')\n","\n","    def train_epoch(self):\n","        \"\"\" Trains the model for 1 epoch of the train iterator\n","\n","        Returns:\n","            tuple: (loss, accuracy)\n","        \"\"\"\n","        epoch_loss = 0\n","        epoch_acc = 0\n","        self.model.train()\n","        for batch in self.train_iter:\n","            # print(batch.__dict__)\n","            self.optimizer.zero_grad()\n","            predictions = self.model(getattr(batch, batch.input_fields[0]))\n","            # Reduce dimentionality if using binary cross entropy loss\n","            if self.criterion_str == \"BCE\":\n","                predictions = predictions.squeeze(1)\n","                acc_formula = binary_accuracy\n","            elif self.criterion_str == \"CE\":\n","                acc_formula = categorical_accuracy\n","            loss = self.criterion(predictions, getattr(batch, batch.target_fields[0]))\n","            acc = acc_formula(predictions, getattr(batch, batch.target_fields[0]))\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","        return epoch_loss / len(self.train_iter), epoch_acc / len(self.train_iter)\n","\n","    def evaluate_epoch(self, iterator):\n","        epoch_loss = 0\n","        epoch_acc = 0\n","        self.model.eval()\n","        with torch.no_grad():\n","            for batch in iterator:\n","                predictions = self.model(getattr(batch, batch.input_fields[0]))\n","                # Reduce dimentionality if using binary cross entropy loss\n","                if self.criterion_str == \"BCE\":\n","                    predictions = predictions.squeeze(1)\n","                    acc_formula = binary_accuracy\n","                elif self.criterion_str == \"CE\":\n","                    acc_formula = categorical_accuracy\n","                loss = self.criterion(predictions, getattr(batch, batch.target_fields[0]))\n","                acc = acc_formula(predictions, getattr(batch, batch.target_fields[0]))\n","\n","                epoch_loss += loss.item()\n","                epoch_acc += acc.item()\n","        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","      \n","    def do_what_we_want(self, num_epochs=10):\n","        self.load_dataset()\n","        self.build_vocab()\n","        self.init_model()\n","        self.init_optimizer()\n","        self.train_model(num_epochs)\n","        self.test_model()\n","\n","\n","def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    # round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float()  # convert into float for division\n","    acc = correct.sum() / len(correct)\n","    return acc\n","\n","\n","def categorical_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    top_pred = preds.argmax(1, keepdim = True)\n","    correct = top_pred.eq(y.view_as(top_pred)).sum()\n","    acc = correct.float() / y.shape[0]\n","    return acc\n","  \n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-hsTbqAgqy9y","executionInfo":{"status":"ok","timestamp":1617584026425,"user_tz":240,"elapsed":544121,"user":{"displayName":"Gavin McClelland","photoUrl":"https://lh4.googleusercontent.com/-GaI7kIqy5pw/AAAAAAAAAAI/AAAAAAAAAGI/xEKkEDRyN00/s64/photo.jpg","userId":"11438477662456870899"}},"outputId":"2e9fc780-dcd4-42ca-e4e1-711a59bf0d23"},"source":["import gc\n","\n","if __name__ == \"__main__\":\n","    # clear contents of file\n","    with open(\"model_results.txt\", \"w\") as f:\n","        pass\n","    \n","\n","    # ======================== FORMALITY BINARY MODEL ========================= #\n","    trainer = ModelTrainer(\n","        model_name=\"CNN-formality-binary\", \n","        text_field_name='Sentence', \n","        label_field_name='Formality', \n","        train_file_name='mturk_news_formality_BINARY.csv', \n","        criterion_str='BCE',\n","        dtype=torch.float)\n","    trainer.do_what_we_want(num_epochs=8)\n","\n","    \n","    # ======================== INFORMATIVENESS BINARY MODEL ========================= #\n","    trainer = ModelTrainer(\n","        model_name=\"CNN-informativeness-binary\", \n","        text_field_name='Sentence', \n","        label_field_name='Informativeness', \n","        train_file_name='mturk_news_informativeness_BINARY.csv', \n","        criterion_str='BCE',\n","        dtype=torch.float)\n","    trainer.do_what_we_want(num_epochs=8)\n","\n","\n","    # ======================== EMOTION MODEL ========================= #\n","    trainer = ModelTrainer(\n","        model_name=\"CNN-emotion\", \n","        text_field_name='Text', \n","        label_field_name='Emotion', \n","        train_file_name='emotion_train.csv', \n","        test_file_name='emotion_test.csv',\n","        criterion_str='CE'\n","    )\n","    trainer.do_what_we_want(num_epochs=8)\n","        # --------------- deep archetecture -------------------#\n","    # trainer = ModelTrainer(\n","    #     model_name=\"deep-CNN-emotion\", \n","    #     model_type='deep', \n","    #     text_field_name='Text', \n","    #     label_field_name='Emotion', \n","    #     train_file_name='/content/emotion_train.csv', \n","    #     test_file_name='/content/emotion_test.csv'\n","    # )\n","    # trainer.do_what_we_want(num_epochs=15)\n","\n","\n","    # ======================== SENTIMENT MODEL ========================= #\n","    trainer = ModelTrainer(\"CNN-sentiment\", IMDB=True, criterion_str=\"BCE\", dtype=torch.float)\n","    trainer.do_what_we_want(num_epochs=8)\n","\n","\n","    # ======================== SARCASM BASE MODEL ========================= #\n","    # trainer = ModelTrainer(\n","    #     model_name=\"CNN-sarcasm-base\", \n","    #     text_field_name='headline', \n","    #     label_field_name='is_sarcastic', \n","    #     train_file_name='Sarcasm_Headlines_Dataset_v2.csv', \n","    #     criterion_str='BCE',\n","    #     dtype=torch.float)\n","    # trainer.do_what_we_want(num_epochs=8)\n","\n","\n","\n","    # emotion_trainer = ModelTrainer(\n","    #     model_name=\"CNN-emotion\", \n","    #     text_field_name='Text', \n","    #     label_field_name='Emotion', \n","    #     train_file_name='emotion_train.csv', \n","    #     test_file_name='emotion_test.csv'\n","    # )\n","    # emotion_trainer.do_what_we_want(num_epochs=8)\n","    # # Clear GPU memory\n","    # emotion_trainer = None\n","    # gc.collect()\n","    # torch.cuda.empty_cache()\n","\n","    # deep_sentament_trainer = ModelTrainer(\"deep-CNN-sentiment\", model_type='deep', IMDB=True, criterion_str=\"BCE\")\n","    # deep_sentament_trainer.do_what_we_want(num_epochs=8)\n","    # # Clear GPU memory\n","    # deep_sentament_trainer = None\n","    # gc.collect()\n","    # torch.cuda.empty_cache()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["\r.vector_cache/glove.6B.zip: 0.00B [00:00, ?B/s]"],"name":"stderr"},{"output_type":"stream","text":["Tesla T4\n"],"name":"stdout"},{"output_type":"stream","text":[".vector_cache/glove.6B.zip: 862MB [02:39, 5.40MB/s]                           \n","100%|█████████▉| 398077/400000 [00:15<00:00, 26980.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["CNN-formality-binary\n","Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.440 | Train Acc: 82.84%\n","\t Val. Loss: 0.321 |  Val. Acc: 90.85%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.285 | Train Acc: 90.21%\n","\t Val. Loss: 0.269 |  Val. Acc: 91.18%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.237 | Train Acc: 90.37%\n","\t Val. Loss: 0.248 |  Val. Acc: 91.18%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.213 | Train Acc: 91.16%\n","\t Val. Loss: 0.235 |  Val. Acc: 91.18%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.186 | Train Acc: 91.93%\n","\t Val. Loss: 0.231 |  Val. Acc: 91.41%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.169 | Train Acc: 92.65%\n","\t Val. Loss: 0.230 |  Val. Acc: 91.29%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.138 | Train Acc: 94.38%\n","\t Val. Loss: 0.229 |  Val. Acc: 91.29%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.114 | Train Acc: 95.75%\n","\t Val. Loss: 0.226 |  Val. Acc: 92.19%\n","Test Loss: 0.231 | Test Acc: 92.31%\n"],"name":"stdout"},{"output_type":"stream","text":["\r100%|█████████▉| 398077/400000 [00:30<00:00, 26980.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["Tesla T4\n","CNN-informativeness-binary\n","Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.400 | Train Acc: 85.26%\n","\t Val. Loss: 0.241 |  Val. Acc: 94.42%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.232 | Train Acc: 93.40%\n","\t Val. Loss: 0.189 |  Val. Acc: 94.42%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.200 | Train Acc: 93.24%\n","\t Val. Loss: 0.174 |  Val. Acc: 94.42%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.183 | Train Acc: 93.16%\n","\t Val. Loss: 0.166 |  Val. Acc: 94.42%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.158 | Train Acc: 93.67%\n","\t Val. Loss: 0.160 |  Val. Acc: 94.42%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.144 | Train Acc: 93.50%\n","\t Val. Loss: 0.155 |  Val. Acc: 94.64%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.134 | Train Acc: 94.38%\n","\t Val. Loss: 0.155 |  Val. Acc: 94.42%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.111 | Train Acc: 95.14%\n","\t Val. Loss: 0.150 |  Val. Acc: 94.75%\n","Test Loss: 0.159 | Test Acc: 94.83%\n","Tesla T4\n","CNN-emotion\n","Epoch: 01 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.359 | Train Acc: 39.82%\n","\t Val. Loss: 1.172 |  Val. Acc: 52.49%\n","Epoch: 02 | Epoch Time: 0m 1s\n","\tTrain Loss: 1.088 | Train Acc: 57.03%\n","\t Val. Loss: 1.029 |  Val. Acc: 60.81%\n","Epoch: 03 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.878 | Train Acc: 67.55%\n","\t Val. Loss: 0.892 |  Val. Acc: 67.05%\n","Epoch: 04 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.689 | Train Acc: 75.56%\n","\t Val. Loss: 0.812 |  Val. Acc: 69.27%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.546 | Train Acc: 81.21%\n","\t Val. Loss: 0.785 |  Val. Acc: 70.41%\n","Epoch: 06 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.455 | Train Acc: 85.01%\n","\t Val. Loss: 0.751 |  Val. Acc: 71.97%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.368 | Train Acc: 88.48%\n","\t Val. Loss: 0.750 |  Val. Acc: 73.08%\n","Epoch: 08 | Epoch Time: 0m 1s\n","\tTrain Loss: 0.309 | Train Acc: 90.44%\n","\t Val. Loss: 0.777 |  Val. Acc: 71.77%\n","Test Loss: 0.761 | Test Acc: 73.99%\n"],"name":"stdout"},{"output_type":"stream","text":["\n","aclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["downloading aclImdb_v1.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["\n","aclImdb_v1.tar.gz:   0%|          | 131k/84.1M [00:00<01:18, 1.07MB/s]\u001b[A\n","aclImdb_v1.tar.gz:   1%|          | 459k/84.1M [00:00<01:02, 1.34MB/s]\u001b[A\n","aclImdb_v1.tar.gz:   2%|▏         | 1.39M/84.1M [00:00<00:46, 1.80MB/s]\u001b[A\n","aclImdb_v1.tar.gz:   4%|▍         | 3.69M/84.1M [00:00<00:32, 2.48MB/s]\u001b[A\n","aclImdb_v1.tar.gz:   8%|▊         | 6.90M/84.1M [00:00<00:22, 3.43MB/s]\u001b[A\n","aclImdb_v1.tar.gz:  13%|█▎        | 11.3M/84.1M [00:00<00:15, 4.74MB/s]\u001b[A\n","aclImdb_v1.tar.gz:  19%|█▉        | 15.8M/84.1M [00:00<00:10, 6.48MB/s]\u001b[A\n","aclImdb_v1.tar.gz:  24%|██▍       | 20.6M/84.1M [00:00<00:07, 8.74MB/s]\u001b[A\n","aclImdb_v1.tar.gz:  30%|███       | 25.6M/84.1M [00:00<00:05, 11.6MB/s]\u001b[A\n","aclImdb_v1.tar.gz:  36%|███▋      | 30.5M/84.1M [00:01<00:03, 15.1MB/s]\u001b[A\n","aclImdb_v1.tar.gz:  43%|████▎     | 35.9M/84.1M [00:01<00:02, 19.2MB/s]\u001b[A\n","aclImdb_v1.tar.gz:  51%|█████     | 42.7M/84.1M [00:01<00:01, 24.5MB/s]\u001b[A\n","aclImdb_v1.tar.gz:  59%|█████▉    | 50.0M/84.1M [00:01<00:01, 30.6MB/s]\u001b[A\n","aclImdb_v1.tar.gz:  67%|██████▋   | 56.7M/84.1M [00:01<00:00, 36.5MB/s]\u001b[A\n","aclImdb_v1.tar.gz:  75%|███████▍  | 62.9M/84.1M [00:01<00:00, 41.3MB/s]\u001b[A\n","aclImdb_v1.tar.gz:  82%|████████▏ | 69.0M/84.1M [00:01<00:00, 45.5MB/s]\u001b[A\n","aclImdb_v1.tar.gz:  89%|████████▉ | 75.0M/84.1M [00:01<00:00, 47.9MB/s]\u001b[A\n","aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 43.9MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Tesla T4\n","CNN-sentiment\n","Epoch: 01 | Epoch Time: 0m 23s\n","\tTrain Loss: 0.654 | Train Acc: 61.26%\n","\t Val. Loss: 0.502 |  Val. Acc: 78.40%\n","Epoch: 02 | Epoch Time: 0m 23s\n","\tTrain Loss: 0.426 | Train Acc: 80.49%\n","\t Val. Loss: 0.353 |  Val. Acc: 84.79%\n","Epoch: 03 | Epoch Time: 0m 23s\n","\tTrain Loss: 0.305 | Train Acc: 87.29%\n","\t Val. Loss: 0.343 |  Val. Acc: 84.62%\n","Epoch: 04 | Epoch Time: 0m 23s\n","\tTrain Loss: 0.222 | Train Acc: 91.31%\n","\t Val. Loss: 0.311 |  Val. Acc: 87.18%\n","Epoch: 05 | Epoch Time: 0m 24s\n","\tTrain Loss: 0.157 | Train Acc: 94.32%\n","\t Val. Loss: 0.338 |  Val. Acc: 86.56%\n","Epoch: 06 | Epoch Time: 0m 24s\n","\tTrain Loss: 0.109 | Train Acc: 96.12%\n","\t Val. Loss: 0.337 |  Val. Acc: 87.38%\n","Epoch: 07 | Epoch Time: 0m 24s\n","\tTrain Loss: 0.072 | Train Acc: 97.76%\n","\t Val. Loss: 0.366 |  Val. Acc: 87.16%\n","Epoch: 08 | Epoch Time: 0m 24s\n","\tTrain Loss: 0.052 | Train Acc: 98.32%\n","\t Val. Loss: 0.397 |  Val. Acc: 87.28%\n","Test Loss: 0.447 | Test Acc: 85.37%\n"],"name":"stdout"}]}]}